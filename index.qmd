---
title: "Exploring Artificial Intelligence and Programming: A Comprehensive Analysis of Developer Perspectives"
author: "Clara Hoey, Daizy Buluma, Christian Manzi"
date: "12 Dec. 2023"
format:
  html:
    theme: cosmo
    toc: true
---

```{r}
#| label: setup
#| include: false

# Set code chunk defaults 
# Consider adding `message = FALSE` option
knitr::opts_chunk$set(echo = FALSE) 
                      
# Set R environment options
options(knitr.kable.NA = '')

# Load packages
library(tidyverse)
library(readr)
library(tidytext)
library(textdata)
library(wordcloud)
```

# Introduction

In the world of coding, things are shaking up with the rise of artificial intelligence (AI) tools. From those just getting the hang of coding to the pros handling big projects, everyone's got a different take on AI. In this blog project, we're digging into the real thoughts of programmers at different career stages. We look into their honest insights about how AI tools are fitting into their coding lives. 

```{r}
#| fig.cap: 'Man vs AI'
#| out.width: 80%
#| fig.align: center

knitr::include_graphics("img/AI.png")
```

As we delve deeper into the dynamic intersection of artificial intelligence and programming, it becomes evident that developers across various career stages are navigating a landscape that continually evolves. From the novice coders navigating their initial lines of code to seasoned professionals orchestrating complex projects, the integration of AI tools is reshaping the very essence of their work. This blog project aims to capture the nuanced perspectives of these developers, shedding light on the multifaceted ways in which AI is influencing their coding practices. Whether it's streamlining repetitive tasks, unlocking new possibilities, or presenting unforeseen challenges, each coder brings a unique story to the table, contributing to the broader narrative of AI's impact on the programming realm.


<!-- ## Website publication work flow -->

<!-- 1. Edit qmd -->

<!-- 2. Knit to HTML to view progress. You may need to click "Open in Browser" for some content to show (sometimes content won't show until you actually push your changes to GitHub and view the published website). -->

<!-- 3. Commit and push changes when you are ready. The website may take a couple minutes to update automatically after the push, but you may need to clear your browser's cache or view the page in a private/incognito window to see the changes more quickly.  -->


# How Developers Describe AI Tools

<!-- You can include text, `code`, and output as usual. Remember to take full advantage of Markdown and its [syntax shortcuts](https://www.markdownguide.org/basic-syntax/). -->

```{r,message=FALSE}
#| label: data-wrangling
#| fig.align: left
#| out.width: 100%

data <- read_csv("data/survey_results_public.csv")

fdata <- data %>% filter(
  Country == "United States of America",
  SOVisitFreq == "Daily or almost daily"
)

#glimpse(data)

saveRDS(data, "survey_results.rds")

#head(data$SOAI)

df <- data |>
  select(ResponseId, SOAI, YearsCode, Age)



saveRDS(df, "data/ID_SOAI_responses.rds")


#################
# TEXT ANALYSIS #
#################

soai_res <- read_rds("data/ID_SOAI_responses.rds")


soai_data <- soai_res |>
  unnest_tokens(output = word, input = SOAI) |>
  # unnest_tokes(input = SOAI, output = bigram, token = "ngrams", n = 3)
  filter(!is.na(word))

#head(soai_data)

#head(stop_words) 

non_stop_words <- c("necessary", "don't", "awfully", "appropriate", "wouldn't") # add more

more_stop_words <- c("stack", "overflow", "ai", "stackoverflow", "questions", "answers", "users")

# filter stop_words to keep some words

filtered_stop_words <- stop_words |>
  filter(!word %in% non_stop_words)

# words in soai_data before filtering for stop_words
#count(soai_data) # 849,989

soai_no_stopwords <- soai_data |>
  anti_join(filtered_stop_words, by = "word") |>
  filter(! word %in% more_stop_words)

#count(soai_no_stopwords) # 345,877

removed_words <- soai_data |>
  anti_join(soai_no_stopwords, by = "word") |>
  count(word) |>
  arrange(word)

#removed_words # 659 words

```
```{r}
#| label: ggplot-1
#| fig.align: left
#| out.width: 100%
# PLOT FREQUENT WORDS

soai_no_stopwords |>
    count(word, sort = TRUE) |>
    slice(1:20) |>
    # fct_reorder is used to re-order the axis (displaying the word)
    # by values of n (the number of times that word was used)
    ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
    geom_col() +
    # Rotate graph
    coord_flip() +
    guides(color = "none",
           fill = "none") +
    labs(
      # Remove x variable label; notice that although coordinates are flipped,
      # the labels correspond to which variables were specified
      # as `x` and `y` in `aes()`
      x = NULL,
      y = "Number of instances",
      title = "The most common words in developers responses on AI")

soai_word_freqs <- soai_no_stopwords |>
  select(word) |>
  count(word)
```
```{r}
#| label: ggplot-2
#| fig.align: left
#| out.width: 100%
#| #| fig.cap: 'Wordcloud'
# WORDCLOUD WITH FREQUENT WORDS

# Word cloud 
  set.seed(1)
  # Create word cloud using tidyverse
  soai_word_freqs |>
    with(wordcloud(words = word,
                   freq = n,
                   min.freq = 1000,
                   max.words = 50))
```

## Overall Developer Sentiments

```{r}
#| label: ggplot-3
#| fig.align: left
#| out.width: 100%

# PLOT OVERALL RESPONSES SENTIMENT SCORE  

afinn_lexicon <- get_sentiments("afinn")

# using afinn lexicon
soai_afinn <- soai_no_stopwords |>
  inner_join(afinn_lexicon, by = "word") |>
  group_by(ResponseId) |>
  summarize(total_score_per_response = sum(value))

ggplot(data = soai_afinn, aes(x = total_score_per_response)) +
  geom_density(color="#AF7AC5", fill="#AF7AC5", alpha = 0.5) +
  theme_classic() +
  labs(title = "Total Response Score based on AFINN Lexicon", x = "Response Score")

```



Examples and additional guidance are provided below. 

Take note of the the default code chunk options in the `setup` code chunk. For example, unlike the rest of the qmd files we worked in this semester, the default code chunk option is `echo = FALSE`, so you will need to set `echo  = TRUE` for any code chunks you would like to display in the blog. You should be thoughtful and intentional about the code you choose to display. 


## Links 

You can include [links](https://www.datadreaming.org/post/r-markdown-theme-gallery/) using Markdown syntax as shown. 

You should include links to relevant sites as you write. You should additionally include a list of references as the end of your blog with full citations (and relevant links).


## Sentiments based on various categories
```{r}
#| label: ds-salaries-app
#| out.width: "100%"


```

<iframe src = "https://manzic.shinyapps.io/NMSHINY/" height=500px width=1000px />



![This is another figure caption](https://media.giphy.com/media/8i5IvxP9ZLgmjXN1BF/giphy.gif)

# Conclusion

Our conclusion and recommendations maybe?

# References

All data sources, any key R packages, and any other sources used in developing your blog should be cited in full in a list of references at the end of your blog. Your blog post should also link to these sources as they are discussed. You may choose any reference style as long as sources are fully cited (try to be consistent!).

Typically, references in R Markdown (and LaTeX) files are [incorporated with a BibTeX database](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html) (a .bib file). You can try this approach or manually include either a numbered or alphabetized list. 

Columbia University has compiled some guidance on [how to cite data](https://guides.library.columbia.edu/datacitation). Some data sources will give you the citation information to copy and paste. Use the provided citations or citation styles in those cases.

You can list R package citations with the code `citation("packageName")` in the console and then copy (and reformat as needed) the relevant text, e.g.,
```{r}
citation("DT")
```

The following citations are based on the American Statistical Association citation style (not all of these references are used in this document).:


<!-- Textbook -->
Baumer, B. S., Kaplan, D. T., and Horton, N. J. (2021), *Modern Data Science with R* (2nd ed.), Boca Raton, FL: CRC Press.

<!-- Journal article -->
Broman, K. W. and Woo, K. H. (2018), "Data Organization in Spreadsheets," *The American Statistician*, 72:1, 2-10, doi: [10.1080/00031305.2017.1375989](https://doi.org/10.1080/00031305.2017.1375989)

<!-- Website -->
Columbia University Libraries (n.d.), "Data Citation," available at <https://guides.library.columbia.edu/datacitation>.

<!-- Journal article -->
McNamara, A. and Horton N. J. (2018) "Wrangling Categorical Data in R," *The American Statistician*, 72:1, 97-104, doi: [10.1080/00031305.2017.1356375](https://doi.org/10.1080/00031305.2017.1356375).

<!-- Dataset -->
Shah, Syed A. A. (October 2022), "Starbucks Drinks" (Version 1), *Kaggle*, available at <https://www.kaggle.com/datasets/syedasimalishah/starbucks-drinks>.

<!-- R package -->
Xie Y, Cheng J, Tan X (2022). "DT: A Wrapper of the JavaScript Library 'DataTables'," R package version 0.24, available at <https://CRAN.R-project.org/package=DT>.
  


 


